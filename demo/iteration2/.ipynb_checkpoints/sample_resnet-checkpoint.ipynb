{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T15:57:47.828678Z",
     "start_time": "2018-12-27T15:57:47.814566Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from torchsummary import summary # pip install torchsummary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Trial1 (Encoder ResNet)\n",
    "- [Code Link](https://github.com/pytorch/vision/blob/v0.2.0/torchvision/models/resnet.py)\n",
    "- [Paper Link](https://arxiv.org/abs/1512.03385)\n",
    "    - Refer Table1 for ResNet-18/34/50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T14:56:59.389655Z",
     "start_time": "2018-12-27T14:56:59.147510Z"
    },
    "code_folding": [
     7,
     23,
     40,
     43,
     55,
     78,
     111,
     131
    ],
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv1x1(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_channels, channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = conv3x3(in_channels, channels, stride)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = conv3x3(channels, channels)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.downsample = downsample\n",
    "        self.stride     = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out      = self.conv1(x)\n",
    "        out      = self.bn1(out)\n",
    "        out      = self.relu(out)\n",
    "        \n",
    "        out      = self.conv2(out)\n",
    "        out      = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        out  += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = conv1x1(inplanes, planes)\n",
    "        self.bn1   = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes, stride)\n",
    "        self.bn2   = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = conv1x1(planes, planes * self.expansion)\n",
    "        self.bn3   = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    \n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, block, layers, num_classes=10, zero_init_residual=False):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.in_channels = 64\n",
    "        self.conv1 = nn.Conv2d(3,64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1   = nn.BatchNorm2d(64)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.fc      = nn.Linear(512*block.expansion, num_classes)\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "        \n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m.bin3.weight, 0):\n",
    "                    nn.init.constant_(m.nbn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight,0)\n",
    "\n",
    "    def _make_layer(self, block, channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        \n",
    "        # print ('Stride : ', stride, ' || self.in_channels :', self.in_channels, ' || Channels : ',channels, block.expansion, channels * block.expansion)\n",
    "        # print (stride != 1, self.in_channels != channels * block.expansion)\n",
    "        if stride != 1 or self.in_channels != channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.in_channels, channels * block.expansion, stride),\n",
    "                nn.BatchNorm2d(channels * block.expansion)                \n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
    "        self.in_channels = channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def forward(self ,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x \n",
    "\n",
    "def resnet18():\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "    return model\n",
    "\n",
    "def resnet34():\n",
    "    model = ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T15:02:03.399257Z",
     "start_time": "2018-12-27T15:01:58.685360Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 256, 256]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 256, 256]             128\n",
      "              ReLU-3         [-1, 64, 256, 256]               0\n",
      "         MaxPool2d-4         [-1, 64, 128, 128]               0\n",
      "            Conv2d-5         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-6         [-1, 64, 128, 128]             128\n",
      "              ReLU-7         [-1, 64, 128, 128]               0\n",
      "            Conv2d-8         [-1, 64, 128, 128]          36,864\n",
      "       BatchNorm2d-9         [-1, 64, 128, 128]             128\n",
      "             ReLU-10         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-11         [-1, 64, 128, 128]               0\n",
      "           Conv2d-12         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-13         [-1, 64, 128, 128]             128\n",
      "             ReLU-14         [-1, 64, 128, 128]               0\n",
      "           Conv2d-15         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-16         [-1, 64, 128, 128]             128\n",
      "             ReLU-17         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-18         [-1, 64, 128, 128]               0\n",
      "           Conv2d-19        [-1, 128, 128, 128]          73,728\n",
      "      BatchNorm2d-20        [-1, 128, 128, 128]             256\n",
      "             ReLU-21        [-1, 128, 128, 128]               0\n",
      "           Conv2d-22        [-1, 128, 128, 128]         147,456\n",
      "      BatchNorm2d-23        [-1, 128, 128, 128]             256\n",
      "           Conv2d-24        [-1, 128, 128, 128]           8,192\n",
      "      BatchNorm2d-25        [-1, 128, 128, 128]             256\n",
      "             ReLU-26        [-1, 128, 128, 128]               0\n",
      "       BasicBlock-27        [-1, 128, 128, 128]               0\n",
      "           Conv2d-28        [-1, 128, 128, 128]         147,456\n",
      "      BatchNorm2d-29        [-1, 128, 128, 128]             256\n",
      "             ReLU-30        [-1, 128, 128, 128]               0\n",
      "           Conv2d-31        [-1, 128, 128, 128]         147,456\n",
      "      BatchNorm2d-32        [-1, 128, 128, 128]             256\n",
      "             ReLU-33        [-1, 128, 128, 128]               0\n",
      "       BasicBlock-34        [-1, 128, 128, 128]               0\n",
      "           Conv2d-35        [-1, 256, 128, 128]         294,912\n",
      "      BatchNorm2d-36        [-1, 256, 128, 128]             512\n",
      "             ReLU-37        [-1, 256, 128, 128]               0\n",
      "           Conv2d-38        [-1, 256, 128, 128]         589,824\n",
      "      BatchNorm2d-39        [-1, 256, 128, 128]             512\n",
      "           Conv2d-40        [-1, 256, 128, 128]          32,768\n",
      "      BatchNorm2d-41        [-1, 256, 128, 128]             512\n",
      "             ReLU-42        [-1, 256, 128, 128]               0\n",
      "       BasicBlock-43        [-1, 256, 128, 128]               0\n",
      "           Conv2d-44        [-1, 256, 128, 128]         589,824\n",
      "      BatchNorm2d-45        [-1, 256, 128, 128]             512\n",
      "             ReLU-46        [-1, 256, 128, 128]               0\n",
      "           Conv2d-47        [-1, 256, 128, 128]         589,824\n",
      "      BatchNorm2d-48        [-1, 256, 128, 128]             512\n",
      "             ReLU-49        [-1, 256, 128, 128]               0\n",
      "       BasicBlock-50        [-1, 256, 128, 128]               0\n",
      "           Conv2d-51        [-1, 512, 128, 128]       1,179,648\n",
      "      BatchNorm2d-52        [-1, 512, 128, 128]           1,024\n",
      "             ReLU-53        [-1, 512, 128, 128]               0\n",
      "           Conv2d-54        [-1, 512, 128, 128]       2,359,296\n",
      "      BatchNorm2d-55        [-1, 512, 128, 128]           1,024\n",
      "           Conv2d-56        [-1, 512, 128, 128]         131,072\n",
      "      BatchNorm2d-57        [-1, 512, 128, 128]           1,024\n",
      "             ReLU-58        [-1, 512, 128, 128]               0\n",
      "       BasicBlock-59        [-1, 512, 128, 128]               0\n",
      "           Conv2d-60        [-1, 512, 128, 128]       2,359,296\n",
      "      BatchNorm2d-61        [-1, 512, 128, 128]           1,024\n",
      "             ReLU-62        [-1, 512, 128, 128]               0\n",
      "           Conv2d-63        [-1, 512, 128, 128]       2,359,296\n",
      "      BatchNorm2d-64        [-1, 512, 128, 128]           1,024\n",
      "             ReLU-65        [-1, 512, 128, 128]               0\n",
      "       BasicBlock-66        [-1, 512, 128, 128]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                   [-1, 10]           5,130\n",
      "================================================================\n",
      "Total params: 11,181,642\n",
      "Trainable params: 11,181,642\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 2008.00\n",
      "Params size (MB): 42.65\n",
      "Estimated Total Size (MB): 2053.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "     \n",
    "    net18 = resnet18().cuda()  # len(list(net18.children())) = 10\n",
    "    # net18 = ResNet(Bottleneck, [2, 2, 2, 2])  # len(list(net18.children())) = 10\n",
    "    summary(net18, input_size=(3, 512, 512))    \n",
    "    # print (len(list(net18.children())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Trial2 (Encoder-Decoder ResNet)\n",
    "- [Link](https://github.com/usuyama/pytorch-unet/blob/master/pytorch_resnet18_unet.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T16:15:11.694500Z",
     "start_time": "2018-12-27T16:15:11.542654Z"
    },
    "code_folding": [],
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "\n",
    "class ResNetUnet(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_classes, verbose=0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.verbose     = verbose \n",
    "        \n",
    "        self.base_model  = models.resnet18(pretrained=False)\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        \n",
    "        self.layer0      = nn.Sequential(*self.base_layers[:3])\n",
    "        self.layer0_1x1  = convrelu(64,64,1,0)\n",
    "        \n",
    "        self.layer1      = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.layer1_1x1  = convrelu(64,64,1,0)\n",
    "        \n",
    "        self.layer2      = self.base_layers[5]\n",
    "        # self.layer2_1x1  = convrelu(512, 512, 1,0)\n",
    "        self.layer2_1x1  = convrelu(128, 64, 1,0)\n",
    "        \n",
    "        self.layer3      = self.base_layers[6]\n",
    "        # self.layer3_1x1  = convrelu(1024,512,1,0)\n",
    "        self.layer3_1x1  = convrelu(256,128,1,0)\n",
    "        \n",
    "        self.layer4      = self.base_layers[7]\n",
    "        # self.layer4_1x1  = convrelu(2048, 1024,1,0)\n",
    "        self.layer4_1x1  = convrelu(512, 256,1,0)\n",
    "        \n",
    "        self.upsample    = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "        # self.conv_up3    = convrelu(512+1024 , 512, 3, 1)\n",
    "        self.conv_up3    = convrelu(128+256 , 128, 3, 1)\n",
    "        #self.conv_up2    = convrelu(512 + 512, 512, 3, 1) 128+64\n",
    "        self.conv_up2    = convrelu(128+64, 64, 3, 1)\n",
    "        #self.conv_up1    = convrelu(256 + 512, 256, 3, 1)\n",
    "        self.conv_up1    = convrelu(64 + 64, 64, 3, 1)\n",
    "        # self.conv_up0    = convrelu(64 + 256 , 128, 3, 1)\n",
    "        self.conv_up0    = convrelu(64 + 64 , 64, 3, 1)\n",
    "        \n",
    "        self.conv_original_size0 = convrelu(3, 32, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(32, 64, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(128, 64, 3, 1)\n",
    "        \n",
    "        self.conv_last   = nn.Conv2d(64, n_classes, 1)\n",
    "    \n",
    "    def forward(self, x_input):\n",
    "        if self.verbose: print ('Preprocess : x_input    : ', x_input.shape)\n",
    "        x_original = self.conv_original_size0(x_input)\n",
    "        if self.verbose: print ('Preprocess : x_original : ', x_original.shape)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "        if self.verbose: print ('Preprocess : x_original : ', x_original.shape)\n",
    "        \n",
    "        \n",
    "        \n",
    "        layer0 = self.layer0(x_input)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print ('\\nEncoder : Layer0 : ', layer0.shape)\n",
    "            print ('Encoder : Layer1 : ', layer1.shape)\n",
    "            print ('Encoder : Layer2 : ', layer2.shape)\n",
    "            print ('Encoder : Layer3 : ', layer3.shape)\n",
    "            print ('Encoder : Layer4 : ', layer4.shape)\n",
    "        \n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        if self.verbose: print ('\\nDecoder : Layer4 : ', layer4.shape)\n",
    "        \n",
    "        x      = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x      = torch.cat([x, layer3], dim=1)\n",
    "        x      = self.conv_up3(x)\n",
    "        if self.verbose: print ('Decoder : Layer3 : ', x.shape)\n",
    "                \n",
    "        x      = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x      = torch.cat([x, layer2], dim=1)\n",
    "        x      = self.conv_up2(x)\n",
    "        if self.verbose: print ('Decoder : Layer2 : ', x.shape)\n",
    "        \n",
    "        x      = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x      = torch.cat([x, layer1], dim=1)\n",
    "        x      = self.conv_up1(x)\n",
    "        if self.verbose: print ('Decoder : Layer1 : ', x.shape) # [1, 64, h/4, w/4]\n",
    "        \n",
    "        x      = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x      = torch.cat([x, layer0], dim=1)\n",
    "        x      = self.conv_up0(x)\n",
    "        if self.verbose:print ('Decoder : Layer0 : ', x.shape) # [1, 64, h/2, w/2]\n",
    "        \n",
    "        x      = self.upsample(x)\n",
    "        x      = torch.cat([x, x_original], dim=1)\n",
    "        x      = self.conv_original_size2(x)\n",
    "        if self.verbose : print ('Decoder : x    : ', x.shape) \n",
    "        \n",
    "        out    = self.conv_last(x)\n",
    "        if self.verbose : print ('Decoder : out  : ', out.shape)\n",
    "        \n",
    "        return F.sigmoid(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T16:15:27.931219Z",
     "start_time": "2018-12-27T16:15:27.215070Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strider/anaconda3/lib/python3.5/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 512, 512]             896\n",
      "              ReLU-2         [-1, 32, 512, 512]               0\n",
      "            Conv2d-3         [-1, 64, 512, 512]          18,496\n",
      "              ReLU-4         [-1, 64, 512, 512]               0\n",
      "            Conv2d-5         [-1, 64, 256, 256]           9,408\n",
      "            Conv2d-6         [-1, 64, 256, 256]           9,408\n",
      "       BatchNorm2d-7         [-1, 64, 256, 256]             128\n",
      "       BatchNorm2d-8         [-1, 64, 256, 256]             128\n",
      "              ReLU-9         [-1, 64, 256, 256]               0\n",
      "             ReLU-10         [-1, 64, 256, 256]               0\n",
      "        MaxPool2d-11         [-1, 64, 128, 128]               0\n",
      "        MaxPool2d-12         [-1, 64, 128, 128]               0\n",
      "           Conv2d-13         [-1, 64, 128, 128]          36,864\n",
      "           Conv2d-14         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-15         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-16         [-1, 64, 128, 128]             128\n",
      "             ReLU-17         [-1, 64, 128, 128]               0\n",
      "             ReLU-18         [-1, 64, 128, 128]               0\n",
      "           Conv2d-19         [-1, 64, 128, 128]          36,864\n",
      "           Conv2d-20         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-21         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-22         [-1, 64, 128, 128]             128\n",
      "             ReLU-23         [-1, 64, 128, 128]               0\n",
      "             ReLU-24         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-25         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-26         [-1, 64, 128, 128]               0\n",
      "           Conv2d-27         [-1, 64, 128, 128]          36,864\n",
      "           Conv2d-28         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-29         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-30         [-1, 64, 128, 128]             128\n",
      "             ReLU-31         [-1, 64, 128, 128]               0\n",
      "             ReLU-32         [-1, 64, 128, 128]               0\n",
      "           Conv2d-33         [-1, 64, 128, 128]          36,864\n",
      "           Conv2d-34         [-1, 64, 128, 128]          36,864\n",
      "      BatchNorm2d-35         [-1, 64, 128, 128]             128\n",
      "      BatchNorm2d-36         [-1, 64, 128, 128]             128\n",
      "             ReLU-37         [-1, 64, 128, 128]               0\n",
      "             ReLU-38         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-39         [-1, 64, 128, 128]               0\n",
      "       BasicBlock-40         [-1, 64, 128, 128]               0\n",
      "           Conv2d-41          [-1, 128, 64, 64]          73,728\n",
      "           Conv2d-42          [-1, 128, 64, 64]          73,728\n",
      "      BatchNorm2d-43          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-44          [-1, 128, 64, 64]             256\n",
      "             ReLU-45          [-1, 128, 64, 64]               0\n",
      "             ReLU-46          [-1, 128, 64, 64]               0\n",
      "           Conv2d-47          [-1, 128, 64, 64]         147,456\n",
      "           Conv2d-48          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-49          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-50          [-1, 128, 64, 64]             256\n",
      "           Conv2d-51          [-1, 128, 64, 64]           8,192\n",
      "           Conv2d-52          [-1, 128, 64, 64]           8,192\n",
      "      BatchNorm2d-53          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-54          [-1, 128, 64, 64]             256\n",
      "             ReLU-55          [-1, 128, 64, 64]               0\n",
      "             ReLU-56          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-57          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-58          [-1, 128, 64, 64]               0\n",
      "           Conv2d-59          [-1, 128, 64, 64]         147,456\n",
      "           Conv2d-60          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-61          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-62          [-1, 128, 64, 64]             256\n",
      "             ReLU-63          [-1, 128, 64, 64]               0\n",
      "             ReLU-64          [-1, 128, 64, 64]               0\n",
      "           Conv2d-65          [-1, 128, 64, 64]         147,456\n",
      "           Conv2d-66          [-1, 128, 64, 64]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 64, 64]             256\n",
      "      BatchNorm2d-68          [-1, 128, 64, 64]             256\n",
      "             ReLU-69          [-1, 128, 64, 64]               0\n",
      "             ReLU-70          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-71          [-1, 128, 64, 64]               0\n",
      "       BasicBlock-72          [-1, 128, 64, 64]               0\n",
      "           Conv2d-73          [-1, 256, 32, 32]         294,912\n",
      "           Conv2d-74          [-1, 256, 32, 32]         294,912\n",
      "      BatchNorm2d-75          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-76          [-1, 256, 32, 32]             512\n",
      "             ReLU-77          [-1, 256, 32, 32]               0\n",
      "             ReLU-78          [-1, 256, 32, 32]               0\n",
      "           Conv2d-79          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-80          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-81          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-82          [-1, 256, 32, 32]             512\n",
      "           Conv2d-83          [-1, 256, 32, 32]          32,768\n",
      "           Conv2d-84          [-1, 256, 32, 32]          32,768\n",
      "      BatchNorm2d-85          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-86          [-1, 256, 32, 32]             512\n",
      "             ReLU-87          [-1, 256, 32, 32]               0\n",
      "             ReLU-88          [-1, 256, 32, 32]               0\n",
      "       BasicBlock-89          [-1, 256, 32, 32]               0\n",
      "       BasicBlock-90          [-1, 256, 32, 32]               0\n",
      "           Conv2d-91          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-92          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-93          [-1, 256, 32, 32]             512\n",
      "      BatchNorm2d-94          [-1, 256, 32, 32]             512\n",
      "             ReLU-95          [-1, 256, 32, 32]               0\n",
      "             ReLU-96          [-1, 256, 32, 32]               0\n",
      "           Conv2d-97          [-1, 256, 32, 32]         589,824\n",
      "           Conv2d-98          [-1, 256, 32, 32]         589,824\n",
      "      BatchNorm2d-99          [-1, 256, 32, 32]             512\n",
      "     BatchNorm2d-100          [-1, 256, 32, 32]             512\n",
      "            ReLU-101          [-1, 256, 32, 32]               0\n",
      "            ReLU-102          [-1, 256, 32, 32]               0\n",
      "      BasicBlock-103          [-1, 256, 32, 32]               0\n",
      "      BasicBlock-104          [-1, 256, 32, 32]               0\n",
      "          Conv2d-105          [-1, 512, 16, 16]       1,179,648\n",
      "          Conv2d-106          [-1, 512, 16, 16]       1,179,648\n",
      "     BatchNorm2d-107          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-108          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-109          [-1, 512, 16, 16]               0\n",
      "            ReLU-110          [-1, 512, 16, 16]               0\n",
      "          Conv2d-111          [-1, 512, 16, 16]       2,359,296\n",
      "          Conv2d-112          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-113          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-114          [-1, 512, 16, 16]           1,024\n",
      "          Conv2d-115          [-1, 512, 16, 16]         131,072\n",
      "          Conv2d-116          [-1, 512, 16, 16]         131,072\n",
      "     BatchNorm2d-117          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-118          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-119          [-1, 512, 16, 16]               0\n",
      "            ReLU-120          [-1, 512, 16, 16]               0\n",
      "      BasicBlock-121          [-1, 512, 16, 16]               0\n",
      "      BasicBlock-122          [-1, 512, 16, 16]               0\n",
      "          Conv2d-123          [-1, 512, 16, 16]       2,359,296\n",
      "          Conv2d-124          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-125          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-126          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-127          [-1, 512, 16, 16]               0\n",
      "            ReLU-128          [-1, 512, 16, 16]               0\n",
      "          Conv2d-129          [-1, 512, 16, 16]       2,359,296\n",
      "          Conv2d-130          [-1, 512, 16, 16]       2,359,296\n",
      "     BatchNorm2d-131          [-1, 512, 16, 16]           1,024\n",
      "     BatchNorm2d-132          [-1, 512, 16, 16]           1,024\n",
      "            ReLU-133          [-1, 512, 16, 16]               0\n",
      "            ReLU-134          [-1, 512, 16, 16]               0\n",
      "      BasicBlock-135          [-1, 512, 16, 16]               0\n",
      "      BasicBlock-136          [-1, 512, 16, 16]               0\n",
      "          Conv2d-137          [-1, 256, 16, 16]         131,328\n",
      "            ReLU-138          [-1, 256, 16, 16]               0\n",
      "        Upsample-139          [-1, 256, 32, 32]               0\n",
      "          Conv2d-140          [-1, 128, 32, 32]          32,896\n",
      "            ReLU-141          [-1, 128, 32, 32]               0\n",
      "          Conv2d-142          [-1, 128, 32, 32]         442,496\n",
      "            ReLU-143          [-1, 128, 32, 32]               0\n",
      "        Upsample-144          [-1, 128, 64, 64]               0\n",
      "          Conv2d-145           [-1, 64, 64, 64]           8,256\n",
      "            ReLU-146           [-1, 64, 64, 64]               0\n",
      "          Conv2d-147           [-1, 64, 64, 64]         110,656\n",
      "            ReLU-148           [-1, 64, 64, 64]               0\n",
      "        Upsample-149         [-1, 64, 128, 128]               0\n",
      "          Conv2d-150         [-1, 64, 128, 128]           4,160\n",
      "            ReLU-151         [-1, 64, 128, 128]               0\n",
      "          Conv2d-152         [-1, 64, 128, 128]          73,792\n",
      "            ReLU-153         [-1, 64, 128, 128]               0\n",
      "        Upsample-154         [-1, 64, 256, 256]               0\n",
      "          Conv2d-155         [-1, 64, 256, 256]           4,160\n",
      "            ReLU-156         [-1, 64, 256, 256]               0\n",
      "          Conv2d-157         [-1, 64, 256, 256]          73,792\n",
      "            ReLU-158         [-1, 64, 256, 256]               0\n",
      "        Upsample-159         [-1, 64, 512, 512]               0\n",
      "          Conv2d-160         [-1, 64, 512, 512]          73,792\n",
      "            ReLU-161         [-1, 64, 512, 512]               0\n",
      "          Conv2d-162          [-1, 1, 512, 512]              65\n",
      "================================================================\n",
      "Total params: 23,327,809\n",
      "Trainable params: 23,327,809\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 1645.00\n",
      "Params size (MB): 88.99\n",
      "Estimated Total Size (MB): 1736.99\n",
      "----------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/strider/anaconda3/lib/python3.5/site-packages/torch/nn/functional.py:1332: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "net = ResNetUnet(1).cuda()\n",
    "summary(net, input_size=(3,512, 512)) \n",
    "\n",
    "\n",
    "# net =  ResNetUnet(1, verbose=1)\n",
    "# x  = torch.randn(1,3,512,512)\n",
    "# y = net(x)\n",
    "# y.shape\n",
    "# layers = list(net.children())\n",
    "# print (len(layers))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-27T15:06:17.644847Z",
     "start_time": "2018-12-27T15:06:17.639069Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "torchvision.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualization (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T17:13:45.194405Z",
     "start_time": "2018-12-22T17:13:45.185018Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T17:15:41.437702Z",
     "start_time": "2018-12-22T17:15:41.267634Z"
    },
    "collapsed": false,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential()\n",
    "model.add_module('W0', nn.Linear(8, 16))\n",
    "model.add_module('tanh', nn.Tanh())\n",
    "model.add_module('W1', nn.Linear(16, 1))\n",
    "\n",
    "x = torch.randn(1,8)\n",
    "\n",
    "make_dot(model(x), params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
