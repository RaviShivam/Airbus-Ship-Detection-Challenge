{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:46:37.667717Z",
     "start_time": "2018-12-24T16:46:37.655146Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import traceback\n",
    "import torchvision\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from skimage.io import imread\n",
    "import pickle\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage import img_as_bool\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "\n",
    "TRAINING_STATS = \"UNETv1_checkpoint/progress.csv\"\n",
    "TRAINED_UNET_MODEL = \"UNETv1_checkpoint/model.pt\"\n",
    "SHIP_DIR = \"/media/shivam/DATA/airbus-tracking/\"\n",
    "TEST_IMAGE_DIR = os.path.join(SHIP_DIR, \"test_v2\")\n",
    "\n",
    "\n",
    "VALIDATION_SIZE = 10\n",
    "VALIDATION_BATCH = 2\n",
    "\n",
    "TRAIN_BATCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:45:05.294651Z",
     "start_time": "2018-12-24T16:45:05.215419Z"
    },
    "code_folding": [
     17,
     25,
     36,
     59,
     69,
     70,
     101,
     109,
     117,
     190,
     193,
     198,
     209,
     224,
     233,
     293
    ]
   },
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    ''' conv -> BN -> relu -> conv -> BN -> relu'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mpconv(x)\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "        \n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, out_ch//2, stride=2)\n",
    "        \n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, (diffX//2, diffX - diffX//2,\n",
    "                        diffY//2, diffY - diffY//2)\n",
    "                  )\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64,  128) #x2\n",
    "        self.down2 = down(128, 256) #x3\n",
    "        self.down3 = down(256, 512) #x4\n",
    "        self.down4 = down(512, 512) #x5\n",
    "        self.up1   = up(1024,256)\n",
    "        self.up2   = up(512,128)\n",
    "        self.up3   = up(256,64)\n",
    "        self.up4   = up(128,64)\n",
    "        self.outc  = outconv(64, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        x = self.up1(x5,x4) # (x5-512d + x4-512d  = 1024d--> 256d)\n",
    "        x = self.up2(x,x3)  # (x-256d + x3 - 256d = 512d --> 128d)\n",
    "        x = self.up3(x, x2) # (x-128d + x2 - 128d = 256d --> 64d)\n",
    "        x = self.up4(x,x1)  # (x-64d  + x1 - 64d  = 128d --> 64d)\n",
    "        x = self.outc(x)    # 64d --> n_classes_D\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class KaggleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ship_dir, use_csv=False, ship_count=5):\n",
    "        self.ship_dir = ship_dir\n",
    "        self.train_image_dir = os.path.join(self.ship_dir, 'train_v2')\n",
    "        self.test_image_dir = os.path.join(self.ship_dir, 'test_v2')\n",
    "        print(\"Starting preprocess\")\n",
    "        self.preprocess(use_csv, ship_count)\n",
    "#         self.preprocess_pickle()\n",
    "        \n",
    "    def preprocess_pickle(self):\n",
    "        with open('all_batches_balancedTrain.pickle', 'rb') as f:\n",
    "            self.all_batches_balancedTrain = pickle.load(f)\n",
    "        with open('all_batches_balancedValid.pickle', 'rb') as f:\n",
    "            self.all_batches_balancedValid = pickle.load(f)\n",
    "\n",
    "    def preprocess(self, use_csv, min_ship_count):\n",
    "        \n",
    "        def sample_ships(in_df, base_rep_val=1500):\n",
    "            if in_df['ships'].values[0]==0:\n",
    "                return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n",
    "            else:\n",
    "                return in_df.sample(base_rep_val, replaice=(in_df.shape[0]<base_rep_val))\n",
    "            \n",
    "        if not use_csv:\n",
    "            masks = pd.read_csv(os.path.join(self.ship_dir, 'train_ship_segmentations_v2.csv'))\n",
    "\n",
    "    #         masks = masks.sample(len(masks)/2)\n",
    "            masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "            unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "            print(\"Reach 1\")\n",
    "            unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "            unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "            print(len(unique_img_ids))\n",
    "            # some files are too small/corrupt\n",
    "            print(\"Reach 1.2\")\n",
    "            unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n",
    "                                                                           os.stat(os.path.join(self.train_image_dir, \n",
    "                                                                                                c_img_id)).st_size/1024)\n",
    "            print(\"Reach 2\")\n",
    "            unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\n",
    "            masks.drop(['ships'], axis=1, inplace=True)\n",
    "            train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                             test_size = 0.3, \n",
    "                             stratify = unique_img_ids['ships'])\n",
    "\n",
    "\n",
    "            print(\"Reach 3\")\n",
    "            train_df = pd.merge(masks, train_ids)\n",
    "            valid_df = pd.merge(masks, valid_ids)\n",
    "            \n",
    "            # train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\n",
    "            train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2)\n",
    "            self.train_df = train_df\n",
    "            self.valid_df = valid_df\n",
    "            print(\"Reach 4\")\n",
    "            balanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\n",
    "            # TODO; save function \n",
    "            balanced_train_df.to_csv(\"balanced_train_df.csv\", index=False)\n",
    "            \n",
    "            self.all_batches_balancedTrain = list(balanced_train_df.groupby('ImageId'))\n",
    "            with open('all_batches_balancedTrain.pickle', 'wb') as f:\n",
    "                # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                pickle.dump(self.all_batches_balancedTrain, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            self.all_batches_balancedValid = list(valid_df.groupby('ImageId'))\n",
    "            with open('all_batches_balancedValid.pickle', 'wb') as f:\n",
    "                # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                pickle.dump(self.all_batches_balancedValid, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        else:\n",
    "            filename_train = 'all_batches_balancedTrain_me_{0}.pickle'.format(min_ship_count)\n",
    "            if os.path.exists(filename_train):\n",
    "                print(\"Using existing files : \", filename_train)\n",
    "                with open(filename_train, 'rb') as f:\n",
    "                    self.all_batches_balancedTrain = pickle.load(f)\n",
    "                with open('all_batches_balancedValid.pickle', 'rb') as f:\n",
    "                    self.all_batches_balancedValid = pickle.load(f)\n",
    "            else:\n",
    "                print(\"Creating new files\")\n",
    "                balanced_train_df = pd.read_csv('balanced_train_df.csv')\n",
    "                ourBalanced_train_df = balanced_train_df[balanced_train_df['grouped_ship_count'] >= min_ship_count]\n",
    "            \n",
    "                self.all_batches_balancedTrain = list(ourBalanced_train_df.groupby('ImageId'))\n",
    "                with open(filename_train, 'wb') as f:\n",
    "                    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                    pickle.dump(self.all_batches_balancedTrain, f, pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "                with open('all_batches_balancedValid.pickle', 'rb') as f:\n",
    "                    self.all_batches_balancedValid = pickle.load(f)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.all_batches_balancedTrain)\n",
    "    \n",
    "    def multi_rle_encode(self, img):\n",
    "        labels = label(img[:, :, 0])\n",
    "        return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "    # ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "    def rle_encode(self, img):\n",
    "        '''\n",
    "        img: numpy array, 1 - mask, 0 - background\n",
    "        Returns run length as string formated\n",
    "        '''\n",
    "        pixels = img.T.flatten()\n",
    "        pixels = np.concatenate([[0], pixels, [0]])\n",
    "        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "        runs[1::2] -= runs[::2]\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "\n",
    "    def rle_decode(self, mask_rle, shape=(768, 768)):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "    def masks_as_image(self, in_mask_list):\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "        #if isinstance(in_mask_list, list):\n",
    "        for mask in in_mask_list:\n",
    "            if isinstance(mask, str):\n",
    "                all_masks += self.rle_decode(mask)\n",
    "        return np.expand_dims(all_masks, -1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        crop_delta = 192\n",
    "        factor = 5\n",
    "        rgb_path = os.path.join(self.train_image_dir, self.all_batches_balancedTrain[idx][0])\n",
    "        c_img = imread(rgb_path)\n",
    "        c_mask = self.masks_as_image( self.all_batches_balancedTrain[idx][1]['EncodedPixels'].values)\n",
    "        \n",
    "        c_img = np.stack(c_img, 0)/255.0\n",
    "        c_mask = np.stack(c_mask, 0)\n",
    "        \n",
    "        h, w, _ = c_mask.shape\n",
    "\n",
    "        # Random crop selection trick\n",
    "        c = 0\n",
    "        while (c < 5): \n",
    "            x1 = np.random.randint(0, h-crop_delta)\n",
    "            x2 = np.random.randint(0, w-crop_delta)\n",
    "            c_mask_s = c_mask[x1:x1+crop_delta, x2:x2+crop_delta, :]\n",
    "            c += 1;\n",
    "            if (np.sum(c_mask_s) > 200):\n",
    "                break\n",
    "            \n",
    "        c_img_s = c_img[x1:x1+crop_delta, x2:x2+crop_delta, :]\n",
    "        \n",
    "        # Resizing image trick (not attempted yet)\n",
    "#         c_img = resize(c_img, (c_img.shape[0] // factor, c_img.shape[1] // factor),\n",
    "#                        anti_aliasing=False)\n",
    "        \n",
    "#         c_mask = resize(c_mask, (c_mask.shape[0] // factor, c_mask.shape[1] // factor),\n",
    "#                        anti_aliasing=False)\n",
    "        c_img = c_img_s.transpose(-1, 0, 1)\n",
    "        c_mask = c_mask_s.transpose(-1, 0, 1)\n",
    "#         print(c_img_s.shape, c_mask_s.shape)\n",
    "#         print(c_img.shape, c_mask.shape)\n",
    "        \n",
    "        return c_img.astype('f'), c_mask.astype('f')\n",
    "    \n",
    "    def extract_image(self, idx, datapath, data):\n",
    "        rgb_path = os.path.join(datapath, data[idx][0])\n",
    "        c_img = imread(rgb_path)\n",
    "        c_mask = self.masks_as_image(data[idx][1]['EncodedPixels'].values)\n",
    "        \n",
    "        c_img = c_img.transpose(-1, 0, 1)\n",
    "        c_mask = c_mask.transpose(-1, 0, 1)\n",
    "        return c_img.astype('f'), c_mask.astype('f')\n",
    "        \n",
    "    def validationset(self, size=10, batch_size=2):\n",
    "        random_batches = [np.random.randint(0, len(self.all_batches_balancedValid)-batch_size) for _ in range(10)]\n",
    "        for i in random_batches:\n",
    "            X = []\n",
    "            Y = []\n",
    "            for j in range(batch_size):\n",
    "                X_temp, y_temp = self.extract_image(i+j, self.train_image_dir, self.all_batches_balancedValid)\n",
    "                X.append(X_temp)\n",
    "                Y.append(y_temp)\n",
    "            X = np.array(X)\n",
    "            Y = np.array(Y)\n",
    "            yield X, Y\n",
    "            \n",
    "        \n",
    "    def show(self, x, y):\n",
    "        f, axarr = plt.subplots(1,2, figsize=(15, 15))\n",
    "\n",
    "        axarr[0].imshow(x.transpose(-1, 1, 0))\n",
    "        axarr[1].imshow(y.transpose(-1, 1, 0)[:, :, 0])\n",
    "\n",
    "\n",
    "    \n",
    "def train(net, criterion, optimizer, epochs, trainLoader):\n",
    "    print ('Training has begun ...')\n",
    "    training_stats = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        val_loss = 0;\n",
    "        # Train with all available data.\n",
    "        print(\"Training in epoch: {}\".format(epoch+1))\n",
    "        tcount = 0\n",
    "        for i, data in enumerate(trainLoader):\n",
    "            tcount += 1\n",
    "            X,Y = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            Y_   = net(X.cuda())\n",
    "            loss = criterion(Y_, Y.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validate after each epoch.\n",
    "        print(\"Validating in epoch: {}\".format(epoch+1))\n",
    "        with torch.no_grad():\n",
    "            vcount = 0 \n",
    "            for X_im, y_im in trainDataLoader.dataset.validationset(VALIDATION_SIZE, VALIDATION_BATCH):\n",
    "                y_pred = net(torch.from_numpy(X_im).cuda())\n",
    "                val_loss += criterion(y_pred, torch.from_numpy(y_im).cuda())\n",
    "            \n",
    "        # Normalize and save\n",
    "        running_loss /= len(trainLoader.dataset)\n",
    "        val_loss /= VALIDATION_SIZE*VALIDATION_BATCH\n",
    "        training_stats.append([running_loss, val_loss])\n",
    "        pd.DataFrame(training_stats).to_csv(TRAINING_STATS, header = ['running_loss', 'val_loss'], index = False)\n",
    "        \n",
    "        # Save model\n",
    "        if (epoch%5==0):\n",
    "            print(\"Saving the model at {} epochs\".format(epoch))\n",
    "            torch.save(net.state_dict(), TRAINED_UNET_MODEL)\n",
    "        \n",
    "        # Empty gpu cache\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Epoch: {}, running loss: {:.4f}, validation loss: {:.4f}\".format(epoch+1, running_loss, val_loss))\n",
    "        \n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.view(num, -1)  # Flatten\n",
    "    m2 = target.view(num, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:45:26.631756Z",
     "start_time": "2018-12-24T16:45:06.504048Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocess\n",
      "Creating new files\n"
     ]
    }
   ],
   "source": [
    "# Load in Dataset\n",
    "ship_dir = '/media/shivam/DATA/airbus-tracking/'\n",
    "trainDataset = KaggleDataset(ship_dir,use_csv=True, ship_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and training the U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:46:43.561151Z",
     "start_time": "2018-12-24T16:46:43.029233Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct UNet\n",
    "gc.collect()\n",
    "reuse = False\n",
    "\n",
    "net = UNet(3, 1).cuda()\n",
    "if reuse:\n",
    "    print(\"Reusing model from: {}\".format(TRAINED_UNET_MODEL))\n",
    "    net.load_state_dict(torch.load(TRAINED_UNET_MODEL))\n",
    "    net.eval()\n",
    "    \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "criterion = dice_coeff\n",
    "\n",
    "# Training the model\n",
    "trainDataLoader   = torch.utils.data.DataLoader(\n",
    "        trainDataset\n",
    "        , batch_size=TRAIN_BATCH,shuffle=True\n",
    "        , num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:40:46.522235Z",
     "start_time": "2018-12-24T16:48:41.549408Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has begun ...\n",
      "Training in epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating in epoch: 1\n",
      "Epoch: 1, running loss: 0.0022, validation loss: 0.0034\n",
      "Training in epoch: 2\n",
      "Validating in epoch: 2\n",
      "Epoch: 2, running loss: 0.0022, validation loss: 0.0072\n",
      "Training in epoch: 3\n",
      "Validating in epoch: 3\n",
      "Epoch: 3, running loss: 0.0018, validation loss: 0.0013\n",
      "Training in epoch: 4\n",
      "Validating in epoch: 4\n",
      "Epoch: 4, running loss: 0.0017, validation loss: 0.0039\n",
      "Training in epoch: 5\n",
      "Validating in epoch: 5\n",
      "Epoch: 5, running loss: 0.0013, validation loss: 0.0005\n",
      "Training in epoch: 6\n",
      "Validating in epoch: 6\n",
      "Epoch: 6, running loss: 0.0013, validation loss: 0.0032\n",
      "Training in epoch: 7\n",
      "Validating in epoch: 7\n",
      "Epoch: 7, running loss: 0.0012, validation loss: 0.0012\n",
      "Training in epoch: 8\n",
      "Validating in epoch: 8\n",
      "Epoch: 8, running loss: 0.0012, validation loss: 0.0004\n",
      "Training in epoch: 9\n",
      "Validating in epoch: 9\n",
      "Epoch: 9, running loss: 0.0011, validation loss: -0.0000\n",
      "Training in epoch: 10\n",
      "Validating in epoch: 10\n",
      "Epoch: 10, running loss: 0.0010, validation loss: 0.0029\n",
      "Training in epoch: 11\n",
      "Validating in epoch: 11\n",
      "Epoch: 11, running loss: 0.0010, validation loss: 0.0010\n",
      "Training in epoch: 12\n",
      "Validating in epoch: 12\n",
      "Epoch: 12, running loss: 0.0010, validation loss: 0.0002\n",
      "Training in epoch: 13\n",
      "Validating in epoch: 13\n",
      "Epoch: 13, running loss: 0.0009, validation loss: 0.0006\n",
      "Training in epoch: 14\n",
      "Validating in epoch: 14\n",
      "Epoch: 14, running loss: 0.0009, validation loss: 0.0054\n",
      "Training in epoch: 15\n",
      "Validating in epoch: 15\n",
      "Epoch: 15, running loss: 0.0008, validation loss: 0.0003\n",
      "Training in epoch: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-17:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shivam/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/shivam/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/shivam/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/shivam/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-4-e5b810a0fb1b>\", line 241, in __getitem__\n",
      "    c_img = np.stack(c_img, 0)/255.0\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0f026e49e8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/shivam/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/shivam/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 23819) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-171355e105a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainDataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-e5b810a0fb1b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, criterion, optimizer, epochs, trainLoader)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mY_\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, 100, trainDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T17:40:59.425215Z",
     "start_time": "2018-12-24T17:40:58.893667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test images found: 15606\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (inc): inconv(\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down1): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down2): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down3): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (down4): down(\n",
       "    (mpconv): Sequential(\n",
       "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (1): double_conv(\n",
       "        (conv): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace)\n",
       "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (5): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up1): up(\n",
       "    (up): Upsample(scale_factor=2, mode=bilinear)\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up2): up(\n",
       "    (up): Upsample(scale_factor=2, mode=bilinear)\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up3): up(\n",
       "    (up): Upsample(scale_factor=2, mode=bilinear)\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (up4): up(\n",
       "    (up): Upsample(scale_factor=2, mode=bilinear)\n",
       "    (conv): double_conv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outc): outconv(\n",
       "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from skimage.morphology import binary_opening, disk\n",
    "import gc; gc.enable() # memory is tight\n",
    "\n",
    "\n",
    "# Encoding functions for found masks\n",
    "def multi_rle_encode(img):\n",
    "    labels = label(img[:, :, 0])\n",
    "    return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_encode(img):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "\n",
    "# Load testing images\n",
    "test_paths = os.listdir(TEST_IMAGE_DIR)\n",
    "print(\"Number of test images found: {}\".format(len(test_paths)))\n",
    "\n",
    "# Load inference model\n",
    "net = UNet(3, 1).cuda()\n",
    "net.load_state_dict(torch.load(TRAINED_UNET_MODEL))\n",
    "net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T18:00:34.341180Z",
     "start_time": "2018-12-24T18:00:04.848271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd82a690569e4f2c83826ff93162edd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=15606), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-12d63ae0e574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#     plt.imshow(cur_seg)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_seg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#     break\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#     print((cur_seg>0.5).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_pred_rows = []\n",
    "for c_img_name in tqdm_notebook(test_paths):\n",
    "    c_path = os.path.join(TEST_IMAGE_DIR, c_img_name)\n",
    "    c_img = imread(c_path)\n",
    "    c_img = c_img.transpose(-1, 0, 1).astype('f')\n",
    "    c_img = np.expand_dims(c_img, 0)/255.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cur_seg = net(torch.from_numpy(c_img).cuda())\n",
    "            \n",
    "    \n",
    "#     plt.imshow(cur_seg)\n",
    "    print(sum(np.unique(cur_seg.cpu().numpy()[0, 0, :, :]) > 0))\n",
    "#     break\n",
    "#     print((cur_seg>0.5).shape)\n",
    "#     print(np.expand_dims(disk(2), -1).transpose(-1, 0, 1).shape)\n",
    "#     print(np.expand_dims(disk(2), -1).reshape(-1, 0, 1))\n",
    "#     break\n",
    "#     cur_seg = binary_opening(cur_seg>0.5, np.expand_dims(disk(2), -1).transpose(-1, 0, 1))\n",
    "#     cur_rles = multi_rle_encode(cur_seg)\n",
    "#     if len(cur_rles)>0:\n",
    "#         for c_rle in cur_rles:\n",
    "#             out_pred_rows += [{'ImageId': c_img_name, 'EncodedPixels': c_rle}]\n",
    "#     else:\n",
    "#         out_pred_rows += [{'ImageId': c_img_name, 'EncodedPixels': None}]\n",
    "#     gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to submission file.\n",
    "submission_df = pd.DataFrame(out_pred_rows)[['ImageId', 'EncodedPixels']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T16:06:56.841890Z",
     "start_time": "2018-12-08T16:06:56.823606Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-08T18:20:14.430Z"
    },
    "code_folding": [
     10
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class KaggleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ship_dir):\n",
    "        self.ship_dir = ship_dir\n",
    "        self.train_image_dir = os.path.join(self.ship_dir, 'train_v2')\n",
    "        self.test_image_dir = os.path.join(self.ship_dir, 'test_v2')\n",
    "        print(\"Starting preprocess\")\n",
    "        self.preprocess_pickle()\n",
    "        \n",
    "    def preprocess_pickle(self):\n",
    "        with open('all_batches_balancedTrain.pickle', 'rb') as f:\n",
    "            self.all_batches_balancedTrain = pickle.load(f)\n",
    "        with open('all_batches_balancedValid.pickle', 'rb') as f:\n",
    "            self.all_batches_balancedValid = pickle.load(f)\n",
    "\n",
    "    def preprocess(self):\n",
    "        \n",
    "        def sample_ships(in_df, base_rep_val=1500):\n",
    "            if in_df['ships'].values[0]==0:\n",
    "                return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n",
    "            else:\n",
    "                return in_df.sample(base_rep_val, replace=(in_df.shape[0]<base_rep_val))\n",
    "        masks = pd.read_csv(os.path.join(self.ship_dir, 'train_ship_segmentations_v2.csv'))\n",
    "        \n",
    "        masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "        unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "        print(\"Reach 1\")\n",
    "        unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "        unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "        # some files are too small/corrupt\n",
    "        print(\"Reach 1.2\")\n",
    "        unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n",
    "                                                                       os.stat(os.path.join(self.train_image_dir, \n",
    "                                                                                            c_img_id)).st_size/1024)\n",
    "        print(\"Reach 2\")\n",
    "        unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\n",
    "        masks.drop(['ships'], axis=1, inplace=True)\n",
    "        train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                         test_size = 0.3, \n",
    "                         stratify = unique_img_ids['ships'])\n",
    "        \n",
    "        \n",
    "        print(\"Reach 3\")\n",
    "        train_df = pd.merge(masks, train_ids)\n",
    "        valid_df = pd.merge(masks, valid_ids)\n",
    "        train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\n",
    "\n",
    "        \n",
    "        print(\"Reach 4\")\n",
    "        balanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\n",
    "        print(\"Creating list\")\n",
    "        self.all_batches_balancedTrain = list(balanced_train_df.groupby('ImageId'))\n",
    "        self.all_batches_balancedValid = list(valid_df.groupby('ImageId'))\n",
    "        \n",
    "        with open('all_batches_balancedTrain.pickle', 'wb') as f:\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(self.all_batches_balancedTrain, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open('all_batches_balancedValid.pickle', 'wb') as f:\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(self.all_batches_balancedValid, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_batches_balancedTrain)\n",
    "    \n",
    "    def multi_rle_encode(self, img):\n",
    "        labels = label(img[:, :, 0])\n",
    "        return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "    # ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "    def rle_encode(self, img):\n",
    "        '''\n",
    "        img: numpy array, 1 - mask, 0 - background\n",
    "        Returns run length as string formated\n",
    "        '''\n",
    "        pixels = img.T.flatten()\n",
    "        pixels = np.concatenate([[0], pixels, [0]])\n",
    "        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "        runs[1::2] -= runs[::2]\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "\n",
    "    def rle_decode(self, mask_rle, shape=(768, 768)):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "    def masks_as_image(self, in_mask_list):\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "        #if isinstance(in_mask_list, list):\n",
    "        for mask in in_mask_list:\n",
    "            if isinstance(mask, str):\n",
    "                all_masks += self.rle_decode(mask)\n",
    "        return np.expand_dims(all_masks, -1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_path = os.path.join(self.train_image_dir, self.all_batches_balancedTrain[idx][0])\n",
    "        c_img = imread(rgb_path)\n",
    "        c_mask = self.masks_as_image( self.all_batches_balancedTrain[idx][1]['EncodedPixels'].values)\n",
    "        \n",
    "        c_img = np.stack(c_img, 0)/255.0\n",
    "        c_mask = np.stack(c_mask, 0)\n",
    "        \n",
    "        c_img = resize(c_img, (c_img.shape[0] / 2, c_img.shape[1] / 2),\n",
    "                       anti_aliasing=True)\n",
    "        \n",
    "        c_mask = resize(c_mask, (c_mask.shape[0] / 2, c_mask.shape[1] / 2),\n",
    "                       anti_aliasing=True)\n",
    "        \n",
    "        c_img = c_img.transpose(-1, 0, 1)\n",
    "        c_mask = c_mask.transpose(-1, 0, 1)\n",
    "        \n",
    "        \n",
    "        return c_img, c_mask\n",
    "\n",
    "    def show(self, x, y):\n",
    "        f, axarr = plt.subplots(1,2, figsize=(15, 15))\n",
    "\n",
    "        axarr[0].imshow(x.transpose(-1, 1, 0))\n",
    "        axarr[1].imshow(y.transpose(-1, 1, 0)[:, :, 0])\n",
    "            \n",
    "\n",
    "if(10):\n",
    "    ship_dir = '/media/shivam/DATA/airbus-tracking/'\n",
    "    trainDataset = KaggleDataset(ship_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-08T18:20:14.432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # x, y = trainDataset[1]\n",
    "\n",
    "# # yp = np.ones_like(y)\n",
    "y = torch.from_numpy(np.random.random((4, 1, 153, 153)))\n",
    "y = torch.from_numpy(np.random.random((4, 1, 153, 153)))\n",
    "yp = torch.from_numpy(np.random.random((4, 1, 153, 153)))\n",
    "\n",
    "# print(dice_coeff(y, yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-22T13:42:19.288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,len(trainDataset.all_batches_balancedTrain))\n",
    "# idx = 57\n",
    "factor = 5\n",
    "print(idx)\n",
    "\n",
    "rgb_path = os.path.join(trainDataset.train_image_dir, trainDataset.all_batches_balancedTrain[idx][0])\n",
    "c_img = imread(rgb_path)\n",
    "c_mask = trainDataset.masks_as_image( trainDataset.all_batches_balancedTrain[idx][1]['EncodedPixels'].values)\n",
    "\n",
    "\n",
    "\n",
    "h, w, _ = c_mask.shape\n",
    "c = 192\n",
    "\n",
    "x1 = np.random.randint(0, h-c)\n",
    "x2 = np.random.randint(0, w-c)\n",
    "\n",
    "c_img_s = c_img[x1:x1+c, x2:x2+c, :]\n",
    "c_mask_s = c_mask[x1:x1+c, x2:x2+c, :]\n",
    "\n",
    "# c_img_s = resize(c_img, (c_img.shape[0] // factor, c_img.shape[1] // factor), anti_aliasing=False)\n",
    "# c_mask_s = img_as_bool(resize(c_mask, (c_mask.shape[0] // factor, c_mask.shape[1] // factor), anti_aliasing=False))\n",
    "# c_mask_s = resize(c_mask, (c_mask.shape[0] // factor, c_mask.shape[1] // factor), anti_aliasing=False)\n",
    "\n",
    "# c_mask_s = resize(c_mask, (c_mask.shape[0] // factor, c_mask.shape[1] // factor), anti_aliasing=False)\n",
    "# c_mask_s[c_mask_s < 0.5] = 0\n",
    "# c_mask_s[c_mask_s >= 0.5] = 1\n",
    "\n",
    "# from torchvision.transforms import RandomCrop\n",
    "\n",
    "# func = RandomCrop(192)\n",
    "\n",
    "# c_mask_s = func(torch.from_numpy(c_mask))\n",
    "\n",
    "print(np.sum(c_mask_s))\n",
    "# print(c_img.shape, c_mask.shape)\n",
    "# print(c_img_s.shape, c_mask_s.shape)\n",
    "f, axarr = plt.subplots(2,2, figsize=(15,15))\n",
    "axarr[0][0].imshow(c_img)\n",
    "axarr[0][1].imshow(c_mask[:,:,0])\n",
    "axarr[1][0].imshow(c_img_s)\n",
    "axarr[1][1].imshow(c_mask_s[:,:,0])\n",
    "\n",
    "# print (trainDataset.all_batches_balancedTrain[idx][1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
