{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:46:37.667717Z",
     "start_time": "2018-12-24T16:46:37.655146Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import traceback\n",
    "import torchvision\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from skimage.io import imread\n",
    "import pickle\n",
    "from skimage.segmentation import mark_boundaries\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.transform import rescale, resize, downscale_local_mean\n",
    "from skimage import img_as_bool\n",
    "\n",
    "\n",
    "torch.backends.cudnn.benchmark=True\n",
    "\n",
    "\n",
    "TRAINING_STATS = \"UNETv1_checkpoint/progress.csv\"\n",
    "TRAINED_UNET_MODEL = \"UNETv1_checkpoint/model.pt\"\n",
    "SHIP_DIR = \"/media/shivam/DATA/airbus-tracking/\"\n",
    "TEST_IMAGE_DIR = os.path.join(SHIP_DIR, \"test_v2\")\n",
    "\n",
    "\n",
    "VALIDATION_SIZE = 10\n",
    "VALIDATION_BATCH = 2\n",
    "\n",
    "TRAIN_BATCH = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:45:05.294651Z",
     "start_time": "2018-12-24T16:45:05.215419Z"
    },
    "code_folding": [
     17,
     25,
     36,
     59,
     69,
     70,
     101,
     109,
     117,
     190,
     193,
     198,
     209,
     224,
     270,
     279,
     293
    ]
   },
   "outputs": [],
   "source": [
    "class double_conv(nn.Module):\n",
    "    ''' conv -> BN -> relu -> conv -> BN -> relu'''\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class inconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(inconv, self).__init__()\n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class down(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(down, self).__init__()\n",
    "        self.mpconv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            double_conv(in_ch, out_ch)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mpconv(x)\n",
    "\n",
    "class up(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, bilinear=True):\n",
    "        super(up, self).__init__()\n",
    "        \n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_ch//2, out_ch//2, stride=2)\n",
    "        \n",
    "        self.conv = double_conv(in_ch, out_ch)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        diffX = x2.size()[2] - x1.size()[2]\n",
    "        diffY = x2.size()[3] - x1.size()[3]\n",
    "        \n",
    "        x1 = F.pad(x1, (diffX//2, diffX - diffX//2,\n",
    "                        diffY//2, diffY - diffY//2)\n",
    "                  )\n",
    "        \n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "class outconv(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super(outconv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_ch, out_ch, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super(UNet, self).__init__()\n",
    "        self.inc = inconv(n_channels, 64)\n",
    "        self.down1 = down(64,  128) #x2\n",
    "        self.down2 = down(128, 256) #x3\n",
    "        self.down3 = down(256, 512) #x4\n",
    "        self.down4 = down(512, 512) #x5\n",
    "        self.up1   = up(1024,256)\n",
    "        self.up2   = up(512,128)\n",
    "        self.up3   = up(256,64)\n",
    "        self.up4   = up(128,64)\n",
    "        self.outc  = outconv(64, n_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        x = self.up1(x5,x4) # (x5-512d + x4-512d  = 1024d--> 256d)\n",
    "        x = self.up2(x,x3)  # (x-256d + x3 - 256d = 512d --> 128d)\n",
    "        x = self.up3(x, x2) # (x-128d + x2 - 128d = 256d --> 64d)\n",
    "        x = self.up4(x,x1)  # (x-64d  + x1 - 64d  = 128d --> 64d)\n",
    "        x = self.outc(x)    # 64d --> n_classes_D\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class KaggleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ship_dir, use_csv=False, ship_count=5):\n",
    "        self.ship_dir = ship_dir\n",
    "        self.train_image_dir = os.path.join(self.ship_dir, 'train_v2')\n",
    "        self.test_image_dir = os.path.join(self.ship_dir, 'test_v2')\n",
    "        print(\"Starting preprocess\")\n",
    "        self.preprocess(use_csv, ship_count)\n",
    "#         self.preprocess_pickle()\n",
    "        \n",
    "    def preprocess_pickle(self):\n",
    "        with open('all_batches_balancedTrain.pickle', 'rb') as f:\n",
    "            self.all_batches_balancedTrain = pickle.load(f)\n",
    "        with open('all_batches_balancedValid.pickle', 'rb') as f:\n",
    "            self.all_batches_balancedValid = pickle.load(f)\n",
    "\n",
    "    def preprocess(self, use_csv, min_ship_count):\n",
    "        \n",
    "        def sample_ships(in_df, base_rep_val=1500):\n",
    "            if in_df['ships'].values[0]==0:\n",
    "                return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n",
    "            else:\n",
    "                return in_df.sample(base_rep_val, replaice=(in_df.shape[0]<base_rep_val))\n",
    "            \n",
    "        if not use_csv:\n",
    "            masks = pd.read_csv(os.path.join(self.ship_dir, 'train_ship_segmentations_v2.csv'))\n",
    "\n",
    "    #         masks = masks.sample(len(masks)/2)\n",
    "            masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "            unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "            print(\"Reach 1\")\n",
    "            unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "            unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "            print(len(unique_img_ids))\n",
    "            # some files are too small/corrupt\n",
    "            print(\"Reach 1.2\")\n",
    "            unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n",
    "                                                                           os.stat(os.path.join(self.train_image_dir, \n",
    "                                                                                                c_img_id)).st_size/1024)\n",
    "            print(\"Reach 2\")\n",
    "            unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\n",
    "            masks.drop(['ships'], axis=1, inplace=True)\n",
    "            train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                             test_size = 0.3, \n",
    "                             stratify = unique_img_ids['ships'])\n",
    "\n",
    "\n",
    "            print(\"Reach 3\")\n",
    "            train_df = pd.merge(masks, train_ids)\n",
    "            valid_df = pd.merge(masks, valid_ids)\n",
    "            \n",
    "            # train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\n",
    "            train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2)\n",
    "            self.train_df = train_df\n",
    "            self.valid_df = valid_df\n",
    "            print(\"Reach 4\")\n",
    "            balanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\n",
    "            # TODO; save function \n",
    "            balanced_train_df.to_csv(\"balanced_train_df.csv\", index=False)\n",
    "            \n",
    "            self.all_batches_balancedTrain = list(balanced_train_df.groupby('ImageId'))\n",
    "            with open('all_batches_balancedTrain.pickle', 'wb') as f:\n",
    "                # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                pickle.dump(self.all_batches_balancedTrain, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            self.all_batches_balancedValid = list(valid_df.groupby('ImageId'))\n",
    "            with open('all_batches_balancedValid.pickle', 'wb') as f:\n",
    "                # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                pickle.dump(self.all_batches_balancedValid, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        else:\n",
    "            filename_train = 'all_batches_balancedTrain_me_{0}.pickle'.format(min_ship_count)\n",
    "            if os.path.exists(filename_train):\n",
    "                print(\"Using existing files : \", filename_train)\n",
    "                with open(filename_train, 'rb') as f:\n",
    "                    self.all_batches_balancedTrain = pickle.load(f)\n",
    "                with open('all_batches_balancedValid.pickle', 'rb') as f:\n",
    "                    self.all_batches_balancedValid = pickle.load(f)\n",
    "            else:\n",
    "                print(\"Creating new files\")\n",
    "                balanced_train_df = pd.read_csv('balanced_train_df.csv')\n",
    "                ourBalanced_train_df = balanced_train_df[balanced_train_df['grouped_ship_count'] >= min_ship_count]\n",
    "            \n",
    "                self.all_batches_balancedTrain = list(ourBalanced_train_df.groupby('ImageId'))\n",
    "                with open(filename_train, 'wb') as f:\n",
    "                    # Pickle the 'data' dictionary using the highest protocol available.\n",
    "                    pickle.dump(self.all_batches_balancedTrain, f, pickle.HIGHEST_PROTOCOL)\n",
    "                    \n",
    "                with open('all_batches_balancedValid.pickle', 'rb') as f:\n",
    "                    self.all_batches_balancedValid = pickle.load(f)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.all_batches_balancedTrain)\n",
    "    \n",
    "    def multi_rle_encode(self, img):\n",
    "        labels = label(img[:, :, 0])\n",
    "        return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "    # ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "    def rle_encode(self, img):\n",
    "        '''\n",
    "        img: numpy array, 1 - mask, 0 - background\n",
    "        Returns run length as string formated\n",
    "        '''\n",
    "        pixels = img.T.flatten()\n",
    "        pixels = np.concatenate([[0], pixels, [0]])\n",
    "        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "        runs[1::2] -= runs[::2]\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "\n",
    "    def rle_decode(self, mask_rle, shape=(768, 768)):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "    def masks_as_image(self, in_mask_list):\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "        #if isinstance(in_mask_list, list):\n",
    "        for mask in in_mask_list:\n",
    "            if isinstance(mask, str):\n",
    "                all_masks += self.rle_decode(mask)\n",
    "        return np.expand_dims(all_masks, -1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        crop_delta = 192\n",
    "        factor = 5\n",
    "        rgb_path = os.path.join(self.train_image_dir, self.all_batches_balancedTrain[idx][0])\n",
    "        c_img = imread(rgb_path)\n",
    "        c_mask = self.masks_as_image( self.all_batches_balancedTrain[idx][1]['EncodedPixels'].values)\n",
    "        \n",
    "        c_img = np.stack(c_img, 0)/255.0\n",
    "        c_mask = np.stack(c_mask, 0)\n",
    "        \n",
    "        h, w, _ = c_mask.shape\n",
    "\n",
    "        # Random crop selection trick\n",
    "        c = 0\n",
    "        while (c < 5): \n",
    "            x1 = np.random.randint(0, h-crop_delta)\n",
    "            x2 = np.random.randint(0, w-crop_delta)\n",
    "            c_mask_s = c_mask[x1:x1+crop_delta, x2:x2+crop_delta, :]\n",
    "            c += 1;\n",
    "            if (np.sum(c_mask_s) > 200):\n",
    "                break\n",
    "            \n",
    "        c_img_s = c_img[x1:x1+crop_delta, x2:x2+crop_delta, :]\n",
    "        \n",
    "        # Resizing image trick (not attempted yet)\n",
    "#         c_img = resize(c_img, (c_img.shape[0] // factor, c_img.shape[1] // factor),\n",
    "#                        anti_aliasing=False)\n",
    "        \n",
    "#         c_mask = resize(c_mask, (c_mask.shape[0] // factor, c_mask.shape[1] // factor),\n",
    "#                        anti_aliasing=False)\n",
    "        c_img = c_img_s.transpose(-1, 0, 1)\n",
    "        c_mask = c_mask_s.transpose(-1, 0, 1)\n",
    "#         print(c_img_s.shape, c_mask_s.shape)\n",
    "#         print(c_img.shape, c_mask.shape)\n",
    "        \n",
    "        return c_img.astype('f'), c_mask.astype('f')\n",
    "    \n",
    "    def extract_image(self, idx, datapath, data):\n",
    "        rgb_path = os.path.join(datapath, data[idx][0])\n",
    "        c_img = imread(rgb_path)\n",
    "        c_mask = self.masks_as_image(data[idx][1]['EncodedPixels'].values)\n",
    "        \n",
    "        c_img = c_img.transpose(-1, 0, 1)\n",
    "        c_mask = c_mask.transpose(-1, 0, 1)\n",
    "        return c_img.astype('f'), c_mask.astype('f')\n",
    "        \n",
    "    def validationset(self, size=10, batch_size=2):\n",
    "        random_batches = [np.random.randint(0, len(self.all_batches_balancedValid)-batch_size) for _ in range(10)]\n",
    "        for i in random_batches:\n",
    "            X = []\n",
    "            Y = []\n",
    "            for j in range(batch_size):\n",
    "                X_temp, y_temp = self.extract_image(i+j, self.train_image_dir, self.all_batches_balancedValid)\n",
    "                X.append(X_temp)\n",
    "                Y.append(y_temp)\n",
    "            X = np.array(X)\n",
    "            Y = np.array(Y)\n",
    "            yield X, Y\n",
    "            \n",
    "        \n",
    "    def show(self, x, y):\n",
    "        f, axarr = plt.subplots(1,2, figsize=(15, 15))\n",
    "\n",
    "        axarr[0].imshow(x.transpose(-1, 1, 0))\n",
    "        axarr[1].imshow(y.transpose(-1, 1, 0)[:, :, 0])\n",
    "\n",
    "\n",
    "    \n",
    "def train(net, criterion, optimizer, epochs, trainLoader):\n",
    "    print ('Training has begun ...')\n",
    "    training_stats = []\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0\n",
    "        val_loss = 0;\n",
    "        # Train with all available data.\n",
    "        print(\"Training in epoch: {}\".format(epoch+1))\n",
    "        tcount = 0\n",
    "        for i, data in enumerate(trainLoader):\n",
    "            tcount += 1\n",
    "            X,Y = data\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            Y_   = net(X.cuda())\n",
    "            loss = criterion(Y_, Y.cuda())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Validate after each epoch.\n",
    "        print(\"Validating in epoch: {}\".format(epoch+1))\n",
    "        with torch.no_grad():\n",
    "            vcount = 0 \n",
    "            for X_im, y_im in trainDataLoader.dataset.validationset(VALIDATION_SIZE, VALIDATION_BATCH):\n",
    "                y_pred = net(torch.from_numpy(X_im).cuda())\n",
    "                val_loss += criterion(y_pred, torch.from_numpy(y_im).cuda())\n",
    "            \n",
    "        # Normalize and save\n",
    "        running_loss /= len(trainLoader.dataset)\n",
    "        val_loss /= VALIDATION_SIZE*VALIDATION_BATCH\n",
    "        training_stats.append([running_loss, val_loss])\n",
    "        pd.DataFrame(training_stats).to_csv(TRAINING_STATS, header = ['running_loss', 'val_loss'], index = False)\n",
    "        \n",
    "        # Save model\n",
    "        if (epoch%5==0):\n",
    "            torch.save(net.state_dict(), TRAINED_UNET_MODEL)\n",
    "        \n",
    "        # Empty gpu cache\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"Epoch: {}, running loss: {:.4f}, validation loss: {:.4f}\".format(epoch+1, running_loss, val_loss))\n",
    "        \n",
    "\n",
    "def dice_coeff(pred, target):\n",
    "    smooth = 1.\n",
    "    num = pred.size(0)\n",
    "    m1 = pred.view(num, -1)  # Flatten\n",
    "    m2 = target.view(num, -1)  # Flatten\n",
    "    intersection = (m1 * m2).sum()\n",
    "\n",
    "    return (2. * intersection + smooth) / (m1.sum() + m2.sum() + smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:45:26.631756Z",
     "start_time": "2018-12-24T16:45:06.504048Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocess\n",
      "Creating new files\n"
     ]
    }
   ],
   "source": [
    "# Load in Dataset\n",
    "ship_dir = '/media/shivam/DATA/airbus-tracking/'\n",
    "trainDataset = KaggleDataset(ship_dir,use_csv=True, ship_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing and training the U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-24T16:46:43.561151Z",
     "start_time": "2018-12-24T16:46:43.029233Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct UNet\n",
    "gc.collect()\n",
    "reuse = False\n",
    "\n",
    "net = UNet(3, 1).cuda()\n",
    "if reuse:\n",
    "    print(\"Reusing model from: {}\".format(TRAINED_UNET_MODEL))\n",
    "    net.load_state_dict(torch.load(TRAINED_UNET_MODEL))\n",
    "    net.eval()\n",
    "    \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "criterion = dice_coeff\n",
    "\n",
    "# Training the model\n",
    "trainDataLoader   = torch.utils.data.DataLoader(\n",
    "        trainDataset\n",
    "        , batch_size=TRAIN_BATCH,shuffle=True\n",
    "        , num_workers=1, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-24T16:48:41.547Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training has begun ...\n",
      "Training in epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shivam/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating in epoch: 1\n",
      "Epoch: 1, running loss: 0.0022, validation loss: 0.0034\n",
      "Training in epoch: 2\n",
      "Validating in epoch: 2\n",
      "Epoch: 2, running loss: 0.0022, validation loss: 0.0072\n",
      "Training in epoch: 3\n",
      "Validating in epoch: 3\n",
      "Epoch: 3, running loss: 0.0018, validation loss: 0.0013\n",
      "Training in epoch: 4\n",
      "Validating in epoch: 4\n",
      "Epoch: 4, running loss: 0.0017, validation loss: 0.0039\n",
      "Training in epoch: 5\n",
      "Validating in epoch: 5\n",
      "Epoch: 5, running loss: 0.0013, validation loss: 0.0005\n",
      "Training in epoch: 6\n",
      "Validating in epoch: 6\n",
      "Epoch: 6, running loss: 0.0013, validation loss: 0.0032\n",
      "Training in epoch: 7\n",
      "Validating in epoch: 7\n",
      "Epoch: 7, running loss: 0.0012, validation loss: 0.0012\n",
      "Training in epoch: 8\n"
     ]
    }
   ],
   "source": [
    "train(net, criterion, optimizer, 100, trainDataLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from skimage.morphology import binary_opening, disk\n",
    "\n",
    "# Load testing images\n",
    "test_paths = os.listdir(TEST_IMAGE_DIR)\n",
    "print(len(test_paths), 'test images found')\n",
    "\n",
    "# Load inference model\n",
    "net = UNet(3, 1).cuda()\n",
    "net.load_state_dict(torch.load(TRAINED_UNET_MODEL))\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST RUN\n",
    "fig, m_axs = plt.subplots(8, 2, figsize = (10, 40))\n",
    "for (ax1, ax2), c_img_name in zip(m_axs, test_paths):\n",
    "    c_path = os.path.join(TEST_IMAGE_DIR, c_img_name)\n",
    "    c_img = imread(c_path)\n",
    "    first_img = np.expand_dims(c_img, 0)/255.0\n",
    "    first_seg = fullres_model.predict(first_img)\n",
    "    ax1.imshow(first_img[0])\n",
    "    ax1.set_title('Image')\n",
    "    ax2.imshow(first_seg[0, :, :, 0], vmin = 0, vmax = 1)\n",
    "    ax2.set_title('Prediction')\n",
    "fig.savefig('test_predictions.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_pred_rows = []\n",
    "for c_img_name in tqdm_notebook(test_paths):\n",
    "    c_path = os.path.join(test_image_dir, c_img_name)\n",
    "    c_img = imread(c_path)\n",
    "    c_img = np.expand_dims(c_img, 0)/255.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cur_seg = net(torch.from_numpy(c_img).cuda())\n",
    "            \n",
    "    cur_seg = binary_opening(cur_seg>0.5, np.expand_dims(disk(2), -1))\n",
    "    cur_rles = multi_rle_encode(cur_seg)\n",
    "    if len(cur_rles)>0:\n",
    "        for c_rle in cur_rles:\n",
    "            out_pred_rows += [{'ImageId': c_img_name, 'EncodedPixels': c_rle}]\n",
    "    else:\n",
    "        out_pred_rows += [{'ImageId': c_img_name, 'EncodedPixels': None}]\n",
    "    gc.collect()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-08T16:06:56.841890Z",
     "start_time": "2018-12-08T16:06:56.823606Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Rough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-08T18:20:14.430Z"
    },
    "code_folding": [
     10
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class KaggleDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, ship_dir):\n",
    "        self.ship_dir = ship_dir\n",
    "        self.train_image_dir = os.path.join(self.ship_dir, 'train_v2')\n",
    "        self.test_image_dir = os.path.join(self.ship_dir, 'test_v2')\n",
    "        print(\"Starting preprocess\")\n",
    "        self.preprocess_pickle()\n",
    "        \n",
    "    def preprocess_pickle(self):\n",
    "        with open('all_batches_balancedTrain.pickle', 'rb') as f:\n",
    "            self.all_batches_balancedTrain = pickle.load(f)\n",
    "        with open('all_batches_balancedValid.pickle', 'rb') as f:\n",
    "            self.all_batches_balancedValid = pickle.load(f)\n",
    "\n",
    "    def preprocess(self):\n",
    "        \n",
    "        def sample_ships(in_df, base_rep_val=1500):\n",
    "            if in_df['ships'].values[0]==0:\n",
    "                return in_df.sample(base_rep_val//3) # even more strongly undersample no ships\n",
    "            else:\n",
    "                return in_df.sample(base_rep_val, replace=(in_df.shape[0]<base_rep_val))\n",
    "        masks = pd.read_csv(os.path.join(self.ship_dir, 'train_ship_segmentations_v2.csv'))\n",
    "        \n",
    "        masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "        unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "        print(\"Reach 1\")\n",
    "        unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "        unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "        # some files are too small/corrupt\n",
    "        print(\"Reach 1.2\")\n",
    "        unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n",
    "                                                                       os.stat(os.path.join(self.train_image_dir, \n",
    "                                                                                            c_img_id)).st_size/1024)\n",
    "        print(\"Reach 2\")\n",
    "        unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb']>50] # keep only 50kb files\n",
    "        masks.drop(['ships'], axis=1, inplace=True)\n",
    "        train_ids, valid_ids = train_test_split(unique_img_ids, \n",
    "                         test_size = 0.3, \n",
    "                         stratify = unique_img_ids['ships'])\n",
    "        \n",
    "        \n",
    "        print(\"Reach 3\")\n",
    "        train_df = pd.merge(masks, train_ids)\n",
    "        valid_df = pd.merge(masks, valid_ids)\n",
    "        train_df['grouped_ship_count'] = train_df['ships'].map(lambda x: (x+1)//2).clip(0, 7)\n",
    "\n",
    "        \n",
    "        print(\"Reach 4\")\n",
    "        balanced_train_df = train_df.groupby('grouped_ship_count').apply(sample_ships)\n",
    "        print(\"Creating list\")\n",
    "        self.all_batches_balancedTrain = list(balanced_train_df.groupby('ImageId'))\n",
    "        self.all_batches_balancedValid = list(valid_df.groupby('ImageId'))\n",
    "        \n",
    "        with open('all_batches_balancedTrain.pickle', 'wb') as f:\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(self.all_batches_balancedTrain, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        with open('all_batches_balancedValid.pickle', 'wb') as f:\n",
    "            # Pickle the 'data' dictionary using the highest protocol available.\n",
    "            pickle.dump(self.all_batches_balancedValid, f, pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.all_batches_balancedTrain)\n",
    "    \n",
    "    def multi_rle_encode(self, img):\n",
    "        labels = label(img[:, :, 0])\n",
    "        return [rle_encode(labels==k) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "    # ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "    def rle_encode(self, img):\n",
    "        '''\n",
    "        img: numpy array, 1 - mask, 0 - background\n",
    "        Returns run length as string formated\n",
    "        '''\n",
    "        pixels = img.T.flatten()\n",
    "        pixels = np.concatenate([[0], pixels, [0]])\n",
    "        runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "        runs[1::2] -= runs[::2]\n",
    "        return ' '.join(str(x) for x in runs)\n",
    "\n",
    "    def rle_decode(self, mask_rle, shape=(768, 768)):\n",
    "        '''\n",
    "        mask_rle: run-length as string formated (start length)\n",
    "        shape: (height,width) of array to return \n",
    "        Returns numpy array, 1 - mask, 0 - background\n",
    "        '''\n",
    "        s = mask_rle.split()\n",
    "        starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "        starts -= 1\n",
    "        ends = starts + lengths\n",
    "        img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "        for lo, hi in zip(starts, ends):\n",
    "            img[lo:hi] = 1\n",
    "        return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "    def masks_as_image(self, in_mask_list):\n",
    "        # Take the individual ship masks and create a single mask array for all ships\n",
    "        all_masks = np.zeros((768, 768), dtype = np.int16)\n",
    "        #if isinstance(in_mask_list, list):\n",
    "        for mask in in_mask_list:\n",
    "            if isinstance(mask, str):\n",
    "                all_masks += self.rle_decode(mask)\n",
    "        return np.expand_dims(all_masks, -1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rgb_path = os.path.join(self.train_image_dir, self.all_batches_balancedTrain[idx][0])\n",
    "        c_img = imread(rgb_path)\n",
    "        c_mask = self.masks_as_image( self.all_batches_balancedTrain[idx][1]['EncodedPixels'].values)\n",
    "        \n",
    "        c_img = np.stack(c_img, 0)/255.0\n",
    "        c_mask = np.stack(c_mask, 0)\n",
    "        \n",
    "        c_img = resize(c_img, (c_img.shape[0] / 2, c_img.shape[1] / 2),\n",
    "                       anti_aliasing=True)\n",
    "        \n",
    "        c_mask = resize(c_mask, (c_mask.shape[0] / 2, c_mask.shape[1] / 2),\n",
    "                       anti_aliasing=True)\n",
    "        \n",
    "        c_img = c_img.transpose(-1, 0, 1)\n",
    "        c_mask = c_mask.transpose(-1, 0, 1)\n",
    "        \n",
    "        \n",
    "        return c_img, c_mask\n",
    "\n",
    "    def show(self, x, y):\n",
    "        f, axarr = plt.subplots(1,2, figsize=(15, 15))\n",
    "\n",
    "        axarr[0].imshow(x.transpose(-1, 1, 0))\n",
    "        axarr[1].imshow(y.transpose(-1, 1, 0)[:, :, 0])\n",
    "            \n",
    "\n",
    "if(10):\n",
    "    ship_dir = '/media/shivam/DATA/airbus-tracking/'\n",
    "    trainDataset = KaggleDataset(ship_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-08T18:20:14.432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # x, y = trainDataset[1]\n",
    "\n",
    "# # yp = np.ones_like(y)\n",
    "y = torch.from_numpy(np.random.random((4, 1, 153, 153)))\n",
    "y = torch.from_numpy(np.random.random((4, 1, 153, 153)))\n",
    "yp = torch.from_numpy(np.random.random((4, 1, 153, 153)))\n",
    "\n",
    "# print(dice_coeff(y, yp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-22T13:42:19.288Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.randint(0,len(trainDataset.all_batches_balancedTrain))\n",
    "# idx = 57\n",
    "factor = 5\n",
    "print(idx)\n",
    "\n",
    "rgb_path = os.path.join(trainDataset.train_image_dir, trainDataset.all_batches_balancedTrain[idx][0])\n",
    "c_img = imread(rgb_path)\n",
    "c_mask = trainDataset.masks_as_image( trainDataset.all_batches_balancedTrain[idx][1]['EncodedPixels'].values)\n",
    "\n",
    "\n",
    "\n",
    "h, w, _ = c_mask.shape\n",
    "c = 192\n",
    "\n",
    "x1 = np.random.randint(0, h-c)\n",
    "x2 = np.random.randint(0, w-c)\n",
    "\n",
    "c_img_s = c_img[x1:x1+c, x2:x2+c, :]\n",
    "c_mask_s = c_mask[x1:x1+c, x2:x2+c, :]\n",
    "\n",
    "# c_img_s = resize(c_img, (c_img.shape[0] // factor, c_img.shape[1] // factor), anti_aliasing=False)\n",
    "# c_mask_s = img_as_bool(resize(c_mask, (c_mask.shape[0] // factor, c_mask.shape[1] // factor), anti_aliasing=False))\n",
    "# c_mask_s = resize(c_mask, (c_mask.shape[0] // factor, c_mask.shape[1] // factor), anti_aliasing=False)\n",
    "\n",
    "# c_mask_s = resize(c_mask, (c_mask.shape[0] // factor, c_mask.shape[1] // factor), anti_aliasing=False)\n",
    "# c_mask_s[c_mask_s < 0.5] = 0\n",
    "# c_mask_s[c_mask_s >= 0.5] = 1\n",
    "\n",
    "# from torchvision.transforms import RandomCrop\n",
    "\n",
    "# func = RandomCrop(192)\n",
    "\n",
    "# c_mask_s = func(torch.from_numpy(c_mask))\n",
    "\n",
    "print(np.sum(c_mask_s))\n",
    "# print(c_img.shape, c_mask.shape)\n",
    "# print(c_img_s.shape, c_mask_s.shape)\n",
    "f, axarr = plt.subplots(2,2, figsize=(15,15))\n",
    "axarr[0][0].imshow(c_img)\n",
    "axarr[0][1].imshow(c_mask[:,:,0])\n",
    "axarr[1][0].imshow(c_img_s)\n",
    "axarr[1][1].imshow(c_mask_s[:,:,0])\n",
    "\n",
    "# print (trainDataset.all_batches_balancedTrain[idx][1])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
